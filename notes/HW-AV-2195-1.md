# HW - Audio Visualizer - Part I

[I. Overview](#overview)

[II. Get Started](#start)

[III. What is an Audio Graph?](#what-is-an-audio-graph)

[XXX. Submission](#submission)

<hr/>

<a id="overview" />

## I. Overview
In Part I of this HW, you will be learning about the HTML5 [WebAudio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API), and how to utilize it to create an audio visualizer. Topics explored:

1. [Audio Routing Graph](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API#Audio_graphs)

![image](_images/_av-images/audio-routing-graph.jpg)

2. [`AnalyserNode`](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode)
  - sampling & bins
  - frequency data - `analyserNode.getByteFrequencyData(data);`
  - waveform data  - `analyserNode.getByteTimeDomainData(data);`
  
3. [JavaScript Typed Arrays](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Typed_arrays) - frequency and waveform data is passed *by reference* to these non-resizable typed arrays.

4. [CORS Restrictions](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) - ("Cross-origin Resource Sharing") means that you can't run the visualizer start files locally off of your hard drive. You might see an error message like this:

**`MediaElementAudioSource outputs zeroes due to CORS access restrictions for file:///Users/...../soundfile.mp3`**

**Solutions to CORS issues:**
- Run the code off of a web server, which you can do by uploading your code to Banjo
- Use an IDE like [Brackets](http://brackets.io) - which creates a local web server ("Live Preview") for you to run your code on
- Visual Studio Code has a similar feature called [Live Server](https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer)
- [You can also create a local web server using Python](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/set_up_a_local_testing_server) on your local machine
- NodeJS can also run a local web server for you: https://www.npmjs.com/package/http-server
- [Firefox Developer Edition](https://www.mozilla.org/en-US/firefox/developer/) (and other browsers) let you turn of CORS restrictions, so you don't need a web server --> https://pointdeveloper.com/how-to-bypass-cors-errors-on-chrome-and-firefox-for-testing/

<hr/>

<a id="start" />

## II. Get Started

1) Create a folder named **web-audio-hw**

2) Download the media files from myCourses and place them in the **web-audio-hw** folder

3) Create **index.html** in the **web-audio-hw** folder - here's the code:

**index.html**

```html
<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8" />
	<title>Audio Visualizer</title>
	<link href="styles/default.css" type="text/css" rel="stylesheet" />
	<script src="./src/loader.js" type="module"></script>
</head>
<body>
<main>
	<canvas width="800" height="400"></canvas>
	<div id="controls">
		<section>
			<button id="playButton" data-playing="no"></button>
			<button id="fsButton">Full Screen</button>
		
			<label>Track: 
				<select id="trackSelect">
					<option value="media/New Adventure Theme.mp3" selected>New Adventure Theme</option>
					<option value="media/Peanuts Theme.mp3">Peanuts Theme</option>
					<option value="media/The Picard Song.mp3">The Picard Song</option>
				</select>
			</label>
		</section>
		
		<section>
			Volume: <input type="range" id="volumeSlider" min="0" max="2" value="1" step="0.01">
			<span id="volumeLabel">???</span>
		</section>
	</div>
</main>
</body>
</html>
```

4) Here's the style sheet - it goes in a folder named **styles**:

**styles/default-styles.css**

```css
body {
	background: #eeeeee;
	font-family: tahoma, verdana, sans serif;
}

canvas {
	margin-left: 1rem;
	margin-top: 1rem;
	margin-bottom: 1rem;
	box-shadow: 4px 4px 8px rgba(0,0,0,0.5);
	background: #fafafa;
}

#controls{
  margin-left: 1rem;
}

#volumeLabel{
  margin-left: 1rem;
}

section{
  margin-bottom: 1rem;
}

#playButton{
  font-size: 1.2rem;
  width: 6rem;
}

button[data-playing="yes"]:after{
  content: "Pause";
}

button[data-playing="no"]:after{
  content: "Play";
}

#fsButton{
  font-size: 1.2rem;
  width: 8rem;
}
```

<hr/>

<a id="what-is-an-audio-graph" /> 

## III. What is an Audio Graph?

- The Web Audio API specification describes a high-level JavaScript API for processing and synthesizing audio in web applications. The top level class of the API is [`AudioContext`](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- To play sounds using web audio, we connect a sound *source* to a *destination*.
- The primary paradigm used by WebAudio is that of an *audio routing graph*, where [`AudioNode`](https://developer.mozilla.org/en-US/docs/Web/API/AudioNode) objects are connected together to define the overall audio rendering:
  - mathematically this is a [Directed Graph](https://en.wikipedia.org/wiki/Directed_graph)

![image](_images/_av-images/???)

### III-A. An audio graph for controlling the volume of a sound

- Below we have an example of an [`AudioContext`](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext) graph with a [`GainNode`](https://developer.mozilla.org/en-US/docs/Web/API/GainNode) (volume) between the source node and the destination node. The [`AudioNode`]((https://developer.mozilla.org/en-US/docs/Web/API/AudioNode) ) instances you place between the source and the destination allow us to manipulate and analyse the audio stream.

![image](_images/_av-images/???)


### III-B. An audio graph for analyzing the frequency distribution of a sound

- In this HW, we will be placing an [`AnalyserNode`](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode) between the source and the destination. This analyser node will not actually change the sound, but it will allow us to analyse it:
  - http://webaudio.github.io/web-audio-api/#the-analysernode-interface

- The [`AnalyserNode`](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode) gives us access to the frequency data of the sound, as well as the waveform data (think “change in values” like an oscilloscope). We will then read and visualize this data by drawing onto our canvas tag.

***\*\* Note the British spelling of “Analyser” \*\****


### III-C. What does the audio frequency data look like?

- Here’s the raw frequency data (byte frequency data) array as seen in the debugger. We’ve asked for 64 samples that evenly sample the frequency range of the sound from 0 to 21050 Hz (21.05 kHz):
  - Array element 0 represents: 0 - 329 Hz
  - Array element 1 represents: 329 - 658 Hz
  - Array element 63 represents: 20721 - 21050 Hz

- The values in the array elements represent the loudness of each frequency bin. They are an average of all of the frequencies in the range of that bin. 

- The range of the values is 0-255, where 0 is no loudness, and 255 is the maximum loudness.
- (You can also request the byte frequency data as percentages if you wish, from 0-1.0)
- Here we are sampling this data 60 frames per second. With most sounds, the contents of this array will therefore change every 1/60th of a second as the <audio> player progresses through the sound.



- Note: You can also get waveform data that represent the change in the frequency bins, similar to what you might see in an oscilloscope.

![image](_images/_av-images/???)





<a id="submission" />

## XXX. Submission

- also see the mycourses.rit.edu dropboxes for due date
